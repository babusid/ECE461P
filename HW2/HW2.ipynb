{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oXEDGbnisgUk"
      },
      "source": [
        "## EE 461P: Data Science Principles  \n",
        "### Homework 2  \n",
        "### Total points: 95\n",
        "### Due: Feb 16, 2023, submitted via Canvas by 11:59 pm  \n",
        "\n",
        "Your homework should be written in a **Jupyter notebook**. You may work in groups of two if you wish. Only one student per team needs to submit the assignment on Canvas.  But be sure to include name and UT eID for both students.  Homework groups will be created and managed through Canvas, so please do not arbitrarily change your homework group. If you do change, let the TAs know.\n",
        "\n",
        "Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting. (%matplotlib inline)\n",
        "\n",
        "### Name(s) and EID(s):\n",
        "\n",
        "1. Sidharth Babu, SNB2593\n",
        "\n",
        "2 (if applicable)."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "oXdxq_q-spA8"
      },
      "source": [
        "# Q1. Bias-Variance Trade-off (20 points)\n",
        "\n",
        "## 1.1 Bias-Variance Decomposition (10 points)\n",
        "Consider a real-valued function $h(x)$. You were trying to estimate this function using a regression model on the dataset $D$ consisting of $(X, Y)$ pairs. Let the output of the training procedure be another function given by $y(x;D)$. This new function $y(x;D)$ depends on $D$ since it was obtained by regressing on $D$.\n",
        "\n",
        "To evaluate how well $y(x;D)$ generalizes we are interested in computing the expected error $E_D [(y(x; D) - h(x))^2]$, where the expectation is over all datasets of the same size as $D$, each obtained by i.i.d. sampling from the underlying joint distribution of X and Y. Show that this expected error decomposes into a bias term plus a variance term that you have seen in lecture slides named '2A dsp regression1.pdf'.\n",
        "\n",
        "Hint: \n",
        "Write\n",
        "$$\n",
        "  (y(x; D) - h(x))^2 = ( y(x; D) - E_D [y(x; D)] + E_D [y(x; D)] - h(x))^2 = \n",
        "$$\n",
        "$$\n",
        "  (y(x; D) - E_D [y(x; D)])^2 + (E_D [y(x; D)] - h(x))^2 - 2 (y(x; D) - E_D [y(x; D)]) (E_D [y(x; D)] - h(x))\n",
        "$$\n",
        "\n",
        "And take expectation over $D$ on both sides"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let y(x|D) = y and h(x) = h \n",
        "\n",
        "$$\n",
        "\n",
        "\\begin{align*}\n",
        "    E[(y-h)^2] &= E[(y-E[y])^2] + E[E[y]^2-2E[y]+h^2] -2E[(y-E[y])(E[y]-h)] &\\\\\n",
        "    &= Var(y) + E[y]^2 - 2E[y]E[h] + E[h]^2 -2E[(y-E[y])(E[y]-h)] &\\\\\n",
        "    &= Var(y) + E[y]^2 - 2E[y]E[h] + E[h]^2 -2E[y]^2 +2E[y][h]+2E[y]^2-2E[h]E[y]&\\\\\n",
        "    &= Var(y) + E[y]^2 + E[h]^2 - 2E[y]E[h] &\\\\\n",
        "    &= Var(y) + (E[y]- E[h])^2 &\\\\\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "$$\n",
        "\n",
        "For a given X:\n",
        "$$\n",
        "\\begin{align*}\n",
        "    E[(y(x|D)-h(x))^2] = Var(y(x|D)) + (E[y(x|D)]- h(x))^2\n",
        "\\end{align*}\n",
        "$$"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "  \n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zkKQnQVHwfcP"
      },
      "source": [
        "## 1.2 Bias-Variance Explanation (5 points)\n",
        "Briefly explain the bias and variance formulas that were derived above and describe how they relate to underfitting/overfitting."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Answer:\n",
        "The formula shows that total error is the sum of squared bias and variance. Intuitively, bias is a measure of how far the mean answer is from the underlying distribution, and measures the \"goodness\" of the model. The variance indicates how sensitive the model is to new / unexpected data, and indicates how general the model is. A high bias indicates that a model is very underfit and not complex enough to capture the actual distribution. High variance indicates that the model is overfit, and is overly complex and not able to generalize to unseen data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgMK6NQVxHii"
      },
      "source": [
        "## 1.3 Underfitting vs Overfitting (5 points)\n",
        "Suppose you have randomly divided the given dataset $D$ into a training dataset and a test dataset. Now you keep on gradually reducing the size of train dataset by moving some points to the test set. As the train set size decreases, what do you\n",
        "expect will happen to the train and test errors? In your answers, consider both the expected values of these two quantities as well the spread (variance) in the values obtained. Justify briefly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBUZiRbsNCa0"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Assuming that the model remains constant, the training error will decrease and the test error will increase. This is because the model has to fit to less data, representing the underlying distribution more sparsely. This will lead to a higher bias and lower variance in the model, and it will perform worse on the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tMml6_T6-4x"
      },
      "source": [
        "# Q2. Log-likelihood (10 points)\n",
        "\n",
        "Consider a linear regression model $y = w.x + ϵ$. Here $x$ is a scalar. The noise  $ϵ$ is IID but depends on $x$ in a way described below:\n",
        "$$\n",
        "ϵ_i ∼ N(0, σ^2) \\quad \\text{if } x_i > 0 \n",
        "$$\n",
        "and   \n",
        "$$\n",
        "ϵ_i ∼ N(0, 4σ^2) \\quad \\text{if } x_i \\leq 0 \n",
        "$$\n",
        "Given $n$ observations ${y_1,y_2, \\dots y_n}$, derive the negative log-likelihood term for this assumed generative model. "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Answer:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Deriving Log Likelihood:\n",
        "$$\n",
        "\\begin{align*}\n",
        "    \\mathcal{L}(Y_n) &= \\prod_{i:x_i>0} N(y_i \\vert wx_0, \\sigma^2) \\prod_{i:x_i<=0} N(y_i \\vert wx_0, 4\\sigma^2) \\\\\n",
        "    l(Y_n) &= \\sum_{i:x_i>0} \\log N(y_i \\vert wx_0, \\sigma^2) + \\sum_{i:x_i<=0} \\log N(y_i \\vert wx_0, 4\\sigma^2) \\\\\n",
        "    &= \\sum_{i:x_i>0} \\log \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(y_i-wx_0)^2}{2\\sigma^2}} + \\sum_{i:x_i<=0} \\log \\frac{1}{\\sqrt{2\\pi4\\sigma^2}}e^{-\\frac{(y_i-wx_0)^2}{2(4\\sigma^2)}} \\\\\n",
        "    &= \\sum_{i:x_i>0} \\log (1) - \\log(\\sqrt{2\\pi\\sigma^2})+ \\log (e^{-\\frac{(y_i-wx_0)^2}{2\\sigma^2}}) + \\sum_{i:x_i<=0} \\log (1) - \\log(\\sqrt{8\\pi\\sigma^2})+ \\log (e^{-\\frac{(y_i-wx_0)^2}{8\\sigma^2}}) \\\\\n",
        "    &= \\sum_{i:x_i>0} -\\frac{1}{2}\\log(2\\pi\\sigma^2) - \\frac{(y_i-wx_0)^2}{2\\sigma^2} + \\sum_{i:x_i<=0} -\\frac{1}{2}\\log(8\\pi\\sigma^2) - \\frac{(y_i-wx_0)^2}{8\\sigma^2} \\\\\n",
        "    &= -\\frac{n}{2}\\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2}\\sum_{i:x_i>0}(y_i-wx_0)^2 - \\frac{n}{2}\\log(8\\pi\\sigma^2) - \\frac{1}{8\\sigma^2}\\sum_{i:x_i<=0}(y_i-wx_0)^2 \\\\\n",
        "    &= -\\frac{n}{2}(\\log(2\\pi\\sigma^2)+\\log(8\\pi\\sigma^2)) - \\frac{1}{2\\sigma^2}\\sum_{i:x_i>0}(y_i-wx_0)^2 - \\frac{1}{8\\sigma^2}\\sum_{i:x_i<=0}(y_i-wx_0)^2 \\\\\n",
        "    &= -\\frac{n}{2}(\\log(2\\pi\\sigma^2*8\\pi\\sigma^2)) - \\frac{1}{2\\sigma^2}\\sum_{i:x_i>0}(y_i-wx_0)^2 - \\frac{1}{8\\sigma^2}\\sum_{i:x_i<=0}(y_i-wx_0)^2 \\\\\n",
        "    &= -\\frac{n}{2}\\log(16\\pi\\sigma^4) - \\frac{1}{2\\sigma^2}\\sum_{i:x_i>0}(y_i-wx_0)^2 - \\frac{1}{8\\sigma^2}\\sum_{i:x_i<=0}(y_i-wx_0)^2 \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Therefore, the NLL is:\n",
        "$$\n",
        "NLL(Y_n) = \\frac{n}{2}\\log(16\\pi\\sigma^4) + \\frac{1}{2\\sigma^2}\\sum_{i:x_i>0}(y_i-wx_0)^2 + \\frac{1}{8\\sigma^2}\\sum_{i:x_i<=0}(y_i-wx_0)^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8at2tjzEnd6e"
      },
      "source": [
        "# Q3. Model Complexity and Bias/Variance Trade-off (35 points)\n",
        "\n",
        "3.1 Load the data given in `all_data_q5.npy` using `numpy.load()` function. This dataset contains the train and test datasets in `(x_train, y_train)` and `(x_test, y_test)` respectively. \n",
        "\n",
        "Now, fit the polynomial models of degrees 1, 5, and 10 on the training data, and print out the mean squared error for train and test datasets for all the models. Essentially, we are trying to fit linear models of this form: $\\hat f(x) = \\beta_0 + \\beta_1x + \\beta_1 x^2 + ... + \\beta_px^p$, where $p$ is the degree of the polynomial. (10 points)\n",
        "\n",
        "Visualise the trained models by making predictions on evenly spaced numbers on x-axis in a fixed range, for eg. you can generate x's by calling `x_all = np.linspace(0, 1, 75).reshape(-1,1)` and call predict on x_all.\n",
        "\n",
        "In the same figure, add the following plots:\n",
        "\n",
        "i) Train data plot : y_train vs x_train\n",
        "\n",
        "ii) Test data plot : y_test vs x_test\n",
        "\n",
        "All the plots must clearly labeled. (10 points)\n",
        "\n",
        "\n",
        "\n",
        "**Tips**: you can use `np.vander(np.squeeze(x_train), deg+1)` to generate the `deg`-degree polynomial vector of `x_train`. For example, `np.vander(np.squeeze(x_train), 3)` gives you the second-degree polynomial of `x_train` and you can call `np.vander` inside the fit method of linear regression. \n",
        "\n",
        "\n",
        "Make use of the starter code we have provided, and fill the `plot_curves` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZR8_J4NttejE"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model as lm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "\n",
        "data_load = np.load('./all_data_q5.npy', allow_pickle=True)\n",
        "x_train = data_load.item().get(\"x_train\")\n",
        "y_train = data_load.item().get(\"y_train\")\n",
        "x_test = data_load.item().get(\"x_test\")\n",
        "y_test = data_load.item().get(\"y_test\")\n",
        "\n",
        "lrp = LinearRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YJpFXQW0FpDf"
      },
      "outputs": [],
      "source": [
        "def plot_curves(x_train, y_train, x_test, y_test):\n",
        "  # Fit polynomial models of degrees 1, 5, 10 to the training data. \n",
        "  # Print out the mean squared error (on both train and test sets) for all the models. \n",
        "  # Plot the data (y_train vs x_train and y_test vs x_test), the fitted models (predictions on x_all by different models vs x_all), and the predictions on the test set (predictions on x_test by different models vs x_test). \n",
        "\n",
        "  # YOUR CODE COMES HERE\n",
        "  print(\"IMPLEMENT ME!\")\n",
        "  return [0, 0, 0], [0, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpTIjFRy05wJ"
      },
      "source": [
        "Fit the different polynomials to the training data and make the plots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-e4QCIhzfXr",
        "outputId": "5578dc79-f493-40c6-9745-ac89c126d29b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IMPLEMENT ME!\n"
          ]
        }
      ],
      "source": [
        "train_rmses_100, test_rmses_100 = plot_curves(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxQ6Am5ktoZN"
      },
      "source": [
        "3.2 Which model gives the best performance (measured by MSE)? Explain in terms of the bias-variance tradeoff. (5 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pgrVHp9uM6-"
      },
      "source": [
        "3.3 Analyse how the training data size affects bias and variance of the models. For this, run the analysis in (a) using 20, 40, 60, 80 and all 100 data points. For each of the three models, plot $log(MSE)$ on train dataset vs the size of the training data and again $log(MSE)$ on test dataset vs the size of the training data. State the trends you see as you change the size of the training data on each of the models and explain why you see them. All plots must be in a single figure and labelled correctly. (10 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foRnHC2P0zC5"
      },
      "source": [
        "Study the effects of the training data size on the bias and variance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRXUPkQ6zdix",
        "outputId": "b2f4e1e6-887d-4a82-eb1b-79f29e669361"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20% data\n",
            "IMPLEMENT ME!\n",
            "40% data\n",
            "IMPLEMENT ME!\n",
            "60% data\n",
            "IMPLEMENT ME!\n",
            "80% data\n",
            "IMPLEMENT ME!\n",
            "100% data\n",
            "IMPLEMENT ME!\n"
          ]
        }
      ],
      "source": [
        "print(\"20% data\")\n",
        "train_rmses_20, test_rmses_20 = plot_curves(x_train[40:60], y_train[40:60], x_test, y_test)\n",
        "print(\"40% data\")\n",
        "train_rmses_40, test_rmses_40 = plot_curves(x_train[30:70], y_train[30:70], x_test, y_test)\n",
        "print(\"60% data\")\n",
        "train_rmses_60, test_rmses_60 = plot_curves(x_train[20:80], y_train[20:80], x_test, y_test)\n",
        "print(\"80% data\")\n",
        "train_rmses_80, test_rmses_80 = plot_curves(x_train[10:90], y_train[10:90], x_test, y_test)\n",
        "print(\"100% data\")\n",
        "train_rmses_100, test_rmses_100 = plot_curves(x_train, y_train, x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HogKf84Fxpt"
      },
      "source": [
        "## Answer:"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "qKaVV0ySySKN"
      },
      "source": [
        "# Q4. Stochastic Gradient Descent (20 points)\n",
        "\n",
        "4.1 In class you studied about SGD wherein instead of computing the \"full\" gradient over all data points $\\nabla f(w) = \\dfrac{1}{N} ∑_{i=1}^{N} \\nabla f_i(w)$, you just estimate the gradient based on one randomly selected data point $\\nabla f_i(w)$ at a time, and use it to move forward in optimization. Show that this \"stochastic gradient\" is an unbiased estimator of the full gradient. (10 points)\n",
        "\n",
        "4.2 Suppose you are using SGD optimization to solve a linear regression problem $y = w^Tx$ to obtain the $w$ that minimizes the mean squared error $E(w) = \\dfrac{1}{N} \\sum_{i=1}^{N} (y_i - w^Tx_i)^2$ . Given that you initialise $w$ as $w_0$, write the one step update equation for $w$. Here $x$ is a vector with $M+1$ components. (10 points)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "hIeOjV9uNDQg"
      },
      "source": [
        "## Answer:\n",
        "4.1) The expectation of the gradient is the same as the gradient of the expectation. Therefore, \n",
        "\n",
        "$E[\\nabla f(w)] = \\nabla E[f(w)] = \\nabla \\frac{1}{N} \\sum_{i}^{N}f_i(w) = \\frac{1}{N} \\sum_{i}^{N} \\nabla f_i(w)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
